{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722b9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load libs \n",
    "\n",
    "from pathlib import Path\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import timm \n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from dinov2_ood_utilities.custom_datasets import CustomizedImageFolder, CustomizedImageFolderForImagenetV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eab77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load general list for wnid to index mapping: Index(wnid) -> pos. in list\n",
    "\n",
    "class_to_index_mapping = []\n",
    "\n",
    "with open('../resources/imagenet_train_class_to_index_mapping.csv', 'r') as class_index_table:\n",
    "    class_index_reader = csv.reader(class_index_table, delimiter=';')\n",
    "    for inet_class, _ in class_index_reader: \n",
    "        class_to_index_mapping.append(inet_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060a3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following transform will be applied: Compose(\n",
      "    Resize(size=592, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(518, 518))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "inet-1k: 50000\n",
      "inet_v2_70: 10000\n",
      "inet_r: 30000\n"
     ]
    }
   ],
   "source": [
    "# prepare datasets and dataloaders \n",
    "\n",
    "timm_model = 'vit_small_patch14_dinov2'\n",
    "timm_model_conf = timm.data.resolve_model_data_config(timm_model)\n",
    "timm_model_conf['input_size'] = (3, 518, 518)\n",
    "timm_transform = timm.data.create_transform(**timm_model_conf, is_training=False)\n",
    "print(f'Following transform will be applied: {timm_transform}')\n",
    "\n",
    "with open('../resources/imagenet_1k_label_order.txt', 'r') as label_order_file:\n",
    "    inet_1k_labels = label_order_file.readlines()\n",
    "    inet_1k_labels = [label_order_line.split()[0] for label_order_line in inet_1k_labels]\n",
    "\n",
    "# datasets \n",
    "\n",
    "inet_v2_70 = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                root='../datasets/imagenetv2-threshold0.7-format-val', \n",
    "                                                transform=timm_transform)\n",
    "inet_v2_mf = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                root='../datasets/imagenetv2-matched-frequency-format-val', \n",
    "                                                transform=timm_transform)\n",
    "inet_v2_top = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                 root='../datasets/imagenetv2-top-images-format-val', \n",
    "                                                 transform=timm_transform)\n",
    "inet_1k_val_resized = CustomizedImageFolder(not_processed_imagenet_classes=inet_1k_labels, \n",
    "                                            root='../datasets/ImageNet1k/imagenet1k/ILSVRC/Data/CLS-LOC/val_sorted', \n",
    "                                            transform=timm_transform)\n",
    "\n",
    "inet_r_labels = os.listdir('/home/stud/afroehli/datasets/ImagenetR_orig/imagenet-r')\n",
    "inet_r = CustomizedImageFolder(not_processed_imagenet_classes=inet_r_labels, \n",
    "                               root='/home/stud/afroehli/datasets/ImagenetR_orig/imagenet-r', \n",
    "                               transform=timm_transform)\n",
    "\n",
    "# dataloaders\n",
    "inet_1k_val_loader = torch.utils.data.DataLoader(dataset=inet_1k_val_resized, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_70_loader = torch.utils.data.DataLoader(dataset=inet_v2_70, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_mf_loader = torch.utils.data.DataLoader(dataset=inet_v2_mf, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_top_loader = torch.utils.data.DataLoader(dataset=inet_v2_top, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_r_loader = torch.utils.data.DataLoader(dataset=inet_r, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f'inet-1k: {len(inet_1k_val_resized)}')\n",
    "print(f'inet_v2_70: {len(inet_v2_70)}')\n",
    "print(f'inet_r: {len(inet_r)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bcafb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stud/afroehli/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model to be used \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')\n",
    "\n",
    "vision_transformer = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "vision_transformer.eval()\n",
    "vision_transformer.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next calculate results for dataset: inet_1k_val_cls_pt_new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 391/391 [15:44<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next calculate results for dataset: inet_r_cls_pt_new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 235/235 [09:32<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# compute new embeddings \n",
    "\n",
    "N_LAYERS = 1\n",
    "store_path_root = '../resources/vit_s_embeddings'\n",
    "\n",
    "inet_1k_val_loader_tuple = (inet_1k_val_loader, f'{store_path_root}/inet_1k_val_cls_pt_new.pkl')\n",
    "inet_v2_70_loader_tuple = (inet_v2_70_loader, f'{store_path_root}/inet_v2_70_cls_pt.pkl')\n",
    "inet_v2_mf_loader_tuple = (inet_v2_mf_loader, f'{store_path_root}/inet_v2_mf_cls_pt.pkl')\n",
    "inet_v2_top_loader_tuple = (inet_v2_top_loader, f'{store_path_root}/inet_v2_top_cls_pt.pkl')\n",
    "inet_r_loader_tuple = (inet_r_loader, f'{store_path_root}/inet_r_cls_pt_new.pkl')\n",
    "\n",
    "dataloaders = [inet_v2_70_loader_tuple, inet_v2_mf_loader_tuple, inet_v2_top_loader_tuple]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for loader, str_path in dataloaders:\n",
    "            \n",
    "        model_results = dict()\n",
    "\n",
    "        print(f'Next calculate results for dataset: {str_path.split('/')[-1].removesuffix('.pkl')}')\n",
    "        print(f'Will be stored under: {str_path}')\n",
    "        \n",
    "        for samples, sample_indices in (pbar := tqdm(loader, ncols=100)):\n",
    "\n",
    "            samples = torch.unbind(samples, dim=0)\n",
    "            # get output of last N_LAYERS\n",
    "            batch_out = []\n",
    "            for sample in [sample.unsqueeze(0).to(device) for sample in samples]:\n",
    "                sample_out = vision_transformer.get_intermediate_layers(sample, N_LAYERS, return_class_token=True)\n",
    "                batch_out.append(sample_out)\n",
    "\n",
    "            # store [(mean-patch-tokens, cls-token), ...]\n",
    "            model_out_converted = []\n",
    "            for sample_out in batch_out:\n",
    "                pt_tokens = sample_out[0][0]\n",
    "                cls_token = sample_out[0][1]\n",
    "                model_out_converted.append((torch.mean(pt_tokens, dim=1).cpu().detach().numpy(), cls_token.cpu().detach().numpy()))\n",
    "\n",
    "            # transform sample-index to wnid-string\n",
    "            wnid_per_sample = [class_to_index_mapping[int(sample_index)] for sample_index in sample_indices]\n",
    "\n",
    "            for n, sample_out_conv in enumerate(model_out_converted):\n",
    "                sample_item_wnid = wnid_per_sample[n]\n",
    "                try: \n",
    "                    model_results[sample_item_wnid].append(sample_out_conv)\n",
    "                except KeyError:\n",
    "                    model_results[sample_item_wnid] = [sample_out_conv]\n",
    "\n",
    "        with open(str_path, 'wb') as pkl_file:\n",
    "            pickle.dump(model_results, pkl_file, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2Pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
