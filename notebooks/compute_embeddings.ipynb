{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load libs \n",
    "\n",
    "from pathlib import Path\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import timm \n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from dinov2_ood_utilities.custom_datasets import CustomizedImageFolder, CustomizedImageFolderForImagenetV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eab77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load general list for wnid to index mapping: Index(wnid) -> pos. in list\n",
    "\n",
    "class_to_index_mapping = []\n",
    "\n",
    "with open('../resources/imagenet_train_class_to_index_mapping.csv', 'r') as class_index_table:\n",
    "    class_index_reader = csv.reader(class_index_table, delimiter=';')\n",
    "    for inet_class, _ in class_index_reader: \n",
    "        class_to_index_mapping.append(inet_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8060a3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before change: {'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.875, 'crop_mode': 'center'}\n",
      "Following transform will be applied: Compose(\n",
      "    Resize(size=592, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(518, 518))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "{'input_size': (3, 518, 518), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.875, 'crop_mode': 'center'}\n",
      "inet-1k: 50000\n",
      "inet_v2_70: 10000\n",
      "inet_1k_train: 1281167\n",
      "inet_r: 30000\n"
     ]
    }
   ],
   "source": [
    "# prepare datasets and dataloaders \n",
    "\n",
    "timm_model = 'vit_small_patch14_dinov2'\n",
    "timm_model_conf = timm.data.resolve_model_data_config(timm_model)\n",
    "print(f'Before change: {timm_model_conf}')\n",
    "timm_model_conf['input_size'] = (3, 518, 518)\n",
    "\n",
    "timm_transform = timm.data.create_transform(**timm_model_conf, is_training=False)\n",
    "\n",
    "print(f'Following transform will be applied: {timm_transform}')\n",
    "print(timm_model_conf)\n",
    "\n",
    "\n",
    "with open('../resources/imagenet_1k_label_order.txt', 'r') as label_order_file:\n",
    "    inet_1k_labels = label_order_file.readlines()\n",
    "    inet_1k_labels = [label_order_line.split()[0] for label_order_line in inet_1k_labels]\n",
    "\n",
    "# datasets \n",
    "\n",
    "inet_v2_70 = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                root='/home/stud/afroehli/datasets/ImagenetV2/imagenetv2-threshold0.7-format-val', \n",
    "                                                transform=timm_transform)\n",
    "inet_v2_mf = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                root='/home/stud/afroehli/datasets/ImagenetV2/imagenetv2-matched-frequency-format-val', \n",
    "                                                transform=timm_transform)\n",
    "inet_v2_top = CustomizedImageFolderForImagenetV2(not_processed_imagenet_classes=inet_1k_labels,\n",
    "                                                 root='/home/stud/afroehli/datasets/ImagenetV2/imagenetv2-top-images-format-val', \n",
    "                                                 transform=timm_transform)\n",
    "inet_1k_val_resized = CustomizedImageFolder(not_processed_imagenet_classes=inet_1k_labels, \n",
    "                                            root='/home/stud/afroehli/datasets/ImageNet1k/imagenet1k/ILSVRC/Data/CLS-LOC/val_sorted', \n",
    "                                            transform=timm_transform)\n",
    "\n",
    "inet_1k_train = CustomizedImageFolder(not_processed_imagenet_classes=inet_1k_labels, \n",
    "                                      root='/home/stud/afroehli/datasets/ImageNet1k/imagenet1k/ILSVRC/Data/CLS-LOC/train', \n",
    "                                      transform=timm_transform)\n",
    "\n",
    "inet_r_path = Path('/home/stud/afroehli/datasets/ImagenetR_orig/imagenet-r')\n",
    "inet_r_path.resolve()\n",
    "inet_r_labels = os.listdir(inet_r_path)\n",
    "inet_r = CustomizedImageFolder(not_processed_imagenet_classes=inet_r_labels, root=inet_r_path, transform=timm_transform)\n",
    "\n",
    "# dataloaders\n",
    "\n",
    "inet_1k_val_loader = torch.utils.data.DataLoader(dataset=inet_1k_val_resized, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_70_loader = torch.utils.data.DataLoader(dataset=inet_v2_70, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_mf_loader = torch.utils.data.DataLoader(dataset=inet_v2_mf, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_v2_top_loader = torch.utils.data.DataLoader(dataset=inet_v2_top, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_1k_train_loader = torch.utils.data.DataLoader(dataset=inet_1k_train, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "inet_r_loader = torch.utils.data.DataLoader(dataset=inet_r, shuffle=False, batch_size=128, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f'inet-1k: {len(inet_1k_val_resized)}')\n",
    "print(f'inet_v2_70: {len(inet_v2_70)}')\n",
    "print(f'inet_1k_train: {len(inet_1k_train)}')\n",
    "print(f'inet_r: {len(inet_r)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcafb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stud/afroehli/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/stud/afroehli/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/stud/afroehli/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/stud/afroehli/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model to be used \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')\n",
    "\n",
    "vision_transformer = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "vision_transformer.eval()\n",
    "vision_transformer.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next calculate results for dataset: inet_1k_val_cls_pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‰                                                              | 6/391 [00:17<18:22,  2.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m batch_out = []\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m [sample.unsqueeze(\u001b[32m0\u001b[39m).to(device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples]:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     sample_out = \u001b[43mvision_transformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_class_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     batch_out.append(sample_out)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# store [last layer (mean-patch-tokens, cls-token), scnd-last layer ...]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:310\u001b[39m, in \u001b[36mDinoVisionTransformer.get_intermediate_layers\u001b[39m\u001b[34m(self, x, n, reshape, return_class_token, norm)\u001b[39m\n\u001b[32m    308\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._get_intermediate_layers_chunked(x, n)\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_intermediate_layers_not_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[32m    312\u001b[39m     outputs = [\u001b[38;5;28mself\u001b[39m.norm(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:279\u001b[39m, in \u001b[36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001b[39m\u001b[34m(self, x, n)\u001b[39m\n\u001b[32m    277\u001b[39m blocks_to_take = \u001b[38;5;28mrange\u001b[39m(total_block_len - n, total_block_len) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m blocks_to_take:\n\u001b[32m    281\u001b[39m         output.append(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# compute new embeddings \n",
    "\n",
    "one_layer_pth = '/home/stud/afroehli/coding/model_results/dinov2_vits14/cls_plus_patch_one_lay'\n",
    "\n",
    "inet_1k_val_loader_tuple = (inet_1k_val_loader, f'{one_layer_pth}/inet_1k_val_cls_pt.pkl')\n",
    "# inet_1k_train_loader_tuple = (inet_1k_train_loader, f'{one_layer_pth}/inet_1k_train_cls_pt.pkl')\n",
    "inet_v2_70_loader_tuple = (inet_v2_70_loader, f'{one_layer_pth}/inet_v2_70_cls_pt.pkl')\n",
    "inet_v2_mf_loader_tuple = (inet_v2_mf_loader, f'{one_layer_pth}/inet_v2_mf_cls_pt.pkl')\n",
    "inet_v2_top_loader_tuple = (inet_v2_top_loader, f'{one_layer_pth}/inet_v2_top_cls_pt.pkl')\n",
    "inet_r_loader_tuple = (inet_r_loader, f'{one_layer_pth}/inet_r_cls_pt.pkl')\n",
    "\n",
    "dataloaders = [inet_1k_val_loader_tuple, inet_v2_70_loader_tuple, inet_v2_mf_loader_tuple, inet_v2_top_loader_tuple, inet_r_loader_tuple]\n",
    "\n",
    "max_layers = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for loader, str_path in dataloaders:\n",
    "            \n",
    "        model_results = dict()\n",
    "\n",
    "        print(f'Next calculate results for dataset: {str_path.split('/')[-1].removesuffix('.pkl')}')\n",
    "\n",
    "        for samples, sample_indices in (pbar := tqdm(loader, ncols=100)):\n",
    "\n",
    "            samples = torch.unbind(samples, dim=0)\n",
    "            # get output of last max_layers\n",
    "            batch_out = []\n",
    "            for sample in [sample.unsqueeze(0).to(device) for sample in samples]:\n",
    "                sample_out = vision_transformer.get_intermediate_layers(sample, max_layers, return_class_token=True)\n",
    "                batch_out.append(sample_out)\n",
    "\n",
    "            # store [last layer (mean-patch-tokens, cls-token), scnd-last layer ...]\n",
    "            model_out_converted = []\n",
    "            for sample_out in batch_out:\n",
    "                proc_out = []\n",
    "                for i in range(max_layers):\n",
    "                    pt_tokens = sample_out[i][0]\n",
    "                    cls_token = sample_out[i][1]\n",
    "                    proc_out.append((torch.mean(pt_tokens, dim=1).cpu().detach().numpy(), cls_token.cpu().detach().numpy()))\n",
    "                model_out_converted.append(proc_out)\n",
    "\n",
    "            # transform sample-index to wnid-string\n",
    "            wnid_per_sample = [class_to_index_mapping[int(sample_index)] for sample_index in sample_indices]\n",
    "\n",
    "            for n, sample_out_conv in enumerate(model_out_converted):\n",
    "                sample_item_wnid = wnid_per_sample[n]\n",
    "                try: \n",
    "                    model_results[sample_item_wnid].append(sample_out_conv)\n",
    "                except KeyError:\n",
    "                    model_results[sample_item_wnid] = [sample_out_conv]\n",
    "\n",
    "        with open(str_path, 'wb') as pkl_file:\n",
    "            pickle.dump(model_results, pkl_file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c5cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incomplete classes: 0\n"
     ]
    }
   ],
   "source": [
    "# only intended for embeddings corresponding to ImageNet-1k training set \n",
    "# check if all embeddings have been computed \n",
    "\n",
    "with open('../resources/imagenet_1k_label_order.txt', 'r') as label_order_file:\n",
    "    inet_1k_labels = label_order_file.readlines()\n",
    "    inet_1k_labels = [label_order_line.split()[0] for label_order_line in inet_1k_labels]\n",
    "    \n",
    "with open('/home/afroehli/coding/model_results/dinov2_vits14/inet_1k_train_timm_trans.pkl', 'rb') as pkl_file:\n",
    "    inet_1k_train_model_results = pickle.load(pkl_file)\n",
    "\n",
    "incomplete_classes = 0\n",
    "for wnid in inet_1k_labels:\n",
    "    expec_embeds = len(os.listdir(f'/home/afroehli/datasets/ImageNet1k/imagenet1k/ILSVRC/Data/CLS-LOC/train/{wnid}'))\n",
    "\n",
    "    if expec_embeds != len(inet_1k_train_model_results[wnid]):\n",
    "        incomplete_classes += 1 \n",
    "\n",
    "print(f'Number of incomplete classes: {incomplete_classes}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2Pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
