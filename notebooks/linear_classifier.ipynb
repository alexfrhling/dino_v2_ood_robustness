{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8b7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from torch import hub, device, cuda, load, Tensor, nn, cat, from_numpy, argmax, mean\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.data import resolve_model_data_config, create_transform\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append(r'/home/stud/afroehli/coding/util_scripts')\n",
    "from utils_dataloading.imagenet_tree import ImagenetSemanticInfo, ImagenetSemanticSubtree\n",
    "\n",
    "device = device('cuda' if cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fd72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop node discovered\n"
     ]
    }
   ],
   "source": [
    "# util objects\n",
    "\n",
    "imagenet_info = ImagenetSemanticInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0651b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/stud/afroehli/datasets/imagenet_v2_label_transform/imagenet_1k_label_order.txt', 'r') as label_order_file:\n",
    "    inet_1k_labels = label_order_file.readlines()\n",
    "    inet_1k_labels = [label_order_line.split()[0] for label_order_line in inet_1k_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4b4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a_b_map = dict()\n",
    "\n",
    "with open('/home/stud/afroehli/coding/dinov2_ood/storage/class_a_class_b_timm_trans.csv', 'r') as class_split_table:\n",
    "    closest_pairs_timm_trans = csv.reader(class_split_table, delimiter=';')\n",
    "\n",
    "    for class_a, class_b in closest_pairs_timm_trans:\n",
    "        class_a_b_map[class_a] = class_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb116ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126944/2504633664.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretr_head = load('/home/stud/afroehli/coding/dinov2_ood/pretrained_heads/dinov2_vits14_linear_head.pth')\n",
      "/tmp/ipykernel_1126944/2504633664.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretr_head_big = load('/home/stud/afroehli/coding/dinov2_ood/pretrained_heads/dinov2_vits14_linear4_head.pth')\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "pretr_head = load('/home/stud/afroehli/coding/dinov2_ood/pretrained_heads/dinov2_vits14_linear_head.pth')\n",
    "pretr_head_big = load('/home/stud/afroehli/coding/dinov2_ood/pretrained_heads/dinov2_vits14_linear4_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b860f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearClassifier(nn.Module): \n",
    "\n",
    "    def __init__(self, in_features = 384, out_features = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features= in_features, out_features=out_features),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     return cat((self.network.forward(x), self.network[0](x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de79edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet for training LinearClassifier\n",
    "# custom dataset: dictonary -> [(embedding, index_as_tensor)]\n",
    "\n",
    "class DictionaryDataset(torch.utils.data.Dataset): \n",
    "    \n",
    "    \"\"\"\n",
    "    Pararms:\n",
    "    1) data: complete dataset provided as dict\n",
    "    2) index_list: list of data_dict-keys, order of keys in list will be used to create a tensor as the expected model-output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: dict, index_list: list[str]): \n",
    "        self.data_dict = data\n",
    "        self.index_list = index_list \n",
    "        self.wnid_list = list(self.data_dict.keys())\n",
    "        self.wnid_iterator = iter(self.wnid_list)\n",
    "        self.instance_per_wnid = [len(self.data_dict[key]) for key in self.wnid_list]\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        total_len = 0 \n",
    "        for key in self.wnid_list:\n",
    "            total_len += len(self.data_dict[key])\n",
    "\n",
    "        return total_len \n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]: \n",
    "\n",
    "        sum_instances = 0\n",
    "        sum_rest = 0\n",
    "\n",
    "        # start with first wnid in list\n",
    "        new_wnid = ''\n",
    "\n",
    "        # iterate until total of all already covered instances is bigger than index of interest\n",
    "        # when class of interest is reached, condition will be false (sum_instances points to first element of next class)\n",
    "        for wnid in self.wnid_list: \n",
    "            if sum_instances > idx:\n",
    "                break \n",
    "            new_wnid = wnid \n",
    "            sum_rest = sum_instances \n",
    "            sum_instances += len(self.data_dict[wnid])\n",
    "\n",
    "        # index within class is needed \n",
    "        # fixed order: [class1, class2, ...]\n",
    "        # when lenght of each previously covered class is known, index within class of interest can be calculated \n",
    "        # sum_rest always represent the total of instances of all covered classes yet, not of interest\n",
    "        idx_within_class = idx - sum_rest\n",
    "\n",
    "        np_arr = self.data_dict[new_wnid][idx_within_class]\n",
    "        data_tensor = torch.tensor(np.array(np_arr))\n",
    "\n",
    "        label_tensor = torch.zeros(len(self.index_list))\n",
    "        # put a one at the position for the expected class \n",
    "        label_tensor[self.index_list.index(new_wnid)] = 1 \n",
    "\n",
    "        return data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad5bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all needed dataloaders\n",
    "\n",
    "dataloaders_dict = dict()\n",
    "\n",
    "for loader_name, loader_path in [('inet_1k_val', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_1k_val_pt_timm_trans.pkl'),\n",
    "                                 ('inet_r', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_r_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_70', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_70_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_mf', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_mf_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_top', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_top_plus_pt_timm_trans.pkl')]:\n",
    "    \n",
    "    with open(loader_path, 'rb') as pkl_fl:\n",
    "        dataloaders_dict[loader_name] = DataLoader(dataset=DictionaryDataset(data=pickle.load(pkl_fl), index_list=inet_1k_labels), \n",
    "                                                   batch_size=128, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0e3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_inet_c = dict()\n",
    "\n",
    "src_path = '/home/stud/afroehli/coding/model_results/dinov2_vits14/imagenet_c'\n",
    "inet_c_cor_types = os.listdir(src_path)\n",
    "\n",
    "for cor_type in inet_c_cor_types:\n",
    "    for sev in range(1, 6):\n",
    "        with open(f'{src_path}/{cor_type}/sev_{sev}.pkl', 'rb') as pkl_fl: \n",
    "            if sev == 1:\n",
    "                dataloaders_inet_c[cor_type] = dict()\n",
    "            dataloaders_inet_c[cor_type][sev] = DataLoader(dataset=DictionaryDataset(pickle.load(pkl_fl), inet_1k_labels), \n",
    "                                                           batch_size=128, shuffle=False, num_workers=8, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b57b6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function \n",
    "# takes: model and list of dataloaders\n",
    "# returns: accuracy for each dataloader\n",
    "\n",
    "def cls_with_patch_one_layer(batch_samples: torch.Tensor) -> torch.Tensor:\n",
    "    return cat([cat((batch_sample[3][1].squeeze(), batch_sample[3][0].squeeze())).unsqueeze(0)\n",
    "                                            for batch_sample in batch_samples])\n",
    "\n",
    "def cls_with_patch_one_layer_inet_c(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return cat([cat((batch_sample[1].squeeze(), batch_sample[0].squeeze())).unsqueeze(0) for batch_sample in batch_samples])\n",
    "\n",
    "def cls_without_patch_one_layer(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return cat([batch_sample[3][1] for batch_sample in batch_samples])\n",
    "\n",
    "def cls_without_patch_one_layer_inet_c(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return cat([batch_sample[1] for batch_sample in batch_samples])\n",
    "\n",
    "def cls_with_patch_four_layers(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return cat([cat(((cat([cls_token for _, cls_token in batch_sample], dim=-1),\n",
    "                    batch_sample[3][0])), dim=-1)\n",
    "                    for batch_sample in batch_samples])\n",
    "\n",
    "def calc_accuracy(model: torch.nn.Sequential, n_layers: int, with_patch: bool, dataloaders: list, is_inet_c_data: bool) -> list: \n",
    "\n",
    "    if n_layers == 1:\n",
    "        if with_patch:\n",
    "            sample_transform = cls_with_patch_one_layer \n",
    "            if is_inet_c_data:\n",
    "                sample_transform = cls_with_patch_one_layer_inet_c \n",
    "        else:\n",
    "            sample_transform = cls_without_patch_one_layer \n",
    "            if is_inet_c_data:\n",
    "                sample_transform = cls_without_patch_one_layer_inet_c\n",
    "    elif n_layers == 4: \n",
    "        sample_transform = cls_with_patch_four_layers \n",
    "\n",
    "    if sample_transform == None:\n",
    "        raise ValueError('SampleTransform function could not be defined')\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for dataloader in (pbar := tqdm(dataloaders, ncols=100)):\n",
    "                  \n",
    "        total_preds = 0\n",
    "        total_preds_true = 0\n",
    "        for batch_samples, batch_labels in dataloader: \n",
    "            batch_samples_dev = sample_transform(batch_samples).to(device)\n",
    "            \n",
    "            model_pred = model(batch_samples_dev).cpu()\n",
    "\n",
    "            is_equal = model_pred.argmax(axis=1) == batch_labels.argmax(axis=1)\n",
    "            total_preds += is_equal.shape[0]\n",
    "            total_preds_true += is_equal.type(torch.float).sum().item()\n",
    "\n",
    "        accuracies.append(total_preds_true / total_preds)\n",
    "\n",
    "    return accuracies \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "324a2fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=1920, out_features=1000, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definition of linear classifiers \n",
    "\n",
    "model_params_one_layer = OrderedDict()\n",
    "model_params_four_layers = OrderedDict()\n",
    "model_params_cls_token = OrderedDict()\n",
    "model_params_one_layer['network.0.weight'] = pretr_head['weight']\n",
    "model_params_one_layer['network.0.bias'] = pretr_head['bias']\n",
    "model_params_cls_token['network.0.weight'] = pretr_head['weight'][:,0:384]\n",
    "model_params_cls_token['network.0.bias'] = pretr_head['bias']\n",
    "model_params_four_layers['network.0.weight'] = pretr_head_big['weight']\n",
    "model_params_four_layers['network.0.bias'] = pretr_head_big['bias']\n",
    "\n",
    "lc_one_layer = LinearClassifier(in_features=768, out_features=1000)\n",
    "lc_cls_token = LinearClassifier(in_features=384, out_features=1000)\n",
    "lc_four_layer = LinearClassifier(in_features=1920, out_features=1000)\n",
    "\n",
    "lc_one_layer.load_state_dict(model_params_one_layer)\n",
    "lc_cls_token.load_state_dict(model_params_cls_token)\n",
    "lc_four_layer.load_state_dict(model_params_four_layers)\n",
    "\n",
    "lc_one_layer.eval()\n",
    "lc_cls_token.eval()\n",
    "lc_four_layer.eval()\n",
    "\n",
    "lc_one_layer.to(device)\n",
    "lc_cls_token.to(device)\n",
    "lc_four_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc4afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 95/95 [11:50<00:00,  7.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Calculate acc for each Imagenet-C Val dataset \n",
    "\n",
    "inet_c_accuracies = dict()\n",
    "inet_c_dloader_list = []\n",
    "\n",
    "for cor_type in dataloaders_inet_c.keys():\n",
    "    for sev in dataloaders_inet_c[cor_type].keys():\n",
    "        inet_c_dloader_list.append(dataloaders_inet_c[cor_type][sev])\n",
    "\n",
    "inet_c_acc_list = calc_accuracy(lc_cls_token, 1, False, inet_c_dloader_list, is_inet_c_data=True)\n",
    "\n",
    "acc_res_i = 0\n",
    "for cor_type in dataloaders_inet_c.keys():\n",
    "    for sev in dataloaders_inet_c[cor_type].keys():\n",
    "        if sev == 1:\n",
    "            inet_c_accuracies[cor_type] = dict()\n",
    "        inet_c_accuracies[cor_type][sev] = inet_c_acc_list[acc_res_i]\n",
    "        acc_res_i += 1 \n",
    "\n",
    "with open('/home/stud/afroehli/coding/dinov2_ood/storage/lc_cls_pretr_inet_c_res.pkl', 'wb') as pkl_fl:\n",
    "    pickle.dump(inet_c_accuracies, pkl_fl, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14aad00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset cor_type  sev          model      acc\n",
      "0  inet-c  spatter    1  lc_with_patch  0.78862\n",
      "1  inet-c  spatter    2  lc_with_patch  0.75822\n",
      "2  inet-c  spatter    3  lc_with_patch  0.71746\n",
      "3  inet-c  spatter    4  lc_with_patch  0.72546\n",
      "4  inet-c  spatter    5  lc_with_patch  0.67486\n",
      "  dataset cor_type  sev   model      acc\n",
      "0  inet-c  spatter    1  lc_cls  0.72488\n",
      "1  inet-c  spatter    2  lc_cls  0.69854\n",
      "2  inet-c  spatter    3  lc_cls  0.66534\n",
      "3  inet-c  spatter    4  lc_cls  0.66916\n",
      "4  inet-c  spatter    5  lc_cls  0.63050\n"
     ]
    }
   ],
   "source": [
    "# one DF with acc of all OOD datasets \n",
    "\n",
    "inet_c_acc_dfs = dict()\n",
    "\n",
    "for model, load_path in [('lc_with_patch', '/home/stud/afroehli/coding/dinov2_ood/storage/lc_one_lay_pretr_inet_c_res.pkl'),\n",
    "              ('lc_cls', '/home/stud/afroehli/coding/dinov2_ood/storage/lc_cls_pretr_inet_c_res.pkl')]: \n",
    "    # load dict with acc of 95 Inet-C Datasets \n",
    "    with open(load_path, 'rb') as pkl_fl:\n",
    "        inet_c_acc_results = pickle.load(pkl_fl) \n",
    "\n",
    "    # create tuples with form: (dataset, cor, sev, model, acc)\n",
    "    inet_c_df_data = []\n",
    "    for cor_type in inet_c_acc_results.keys(): \n",
    "        for sev in inet_c_acc_results[cor_type].keys(): \n",
    "            acc = inet_c_acc_results[cor_type][sev]\n",
    "            inet_c_df_data.append(('inet-c', cor_type, sev, model, acc))\n",
    "\n",
    "    # create DF \n",
    "    inet_c_acc_dfs[model] = inet_c_acc_df_with_patch = pd.DataFrame(data=inet_c_df_data, columns=['dataset', 'cor_type', 'sev', 'model', 'acc'])\n",
    "    print(inet_c_acc_dfs[model].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e15f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     patch-acc   cls-acc       dataset\n",
      "sev                                   \n",
      "1     0.739889  0.677464  inet-c-sev-1\n",
      "2     0.673617  0.615355  inet-c-sev-2\n",
      "3     0.599708  0.548516  inet-c-sev-3\n",
      "4     0.497449  0.456148  inet-c-sev-4\n",
      "5     0.367285  0.338309  inet-c-sev-5\n"
     ]
    }
   ],
   "source": [
    "# bring inet-c-acc-DF in form (datasetname + sev, model-1-acc, model-2-acc)\n",
    "\n",
    "\n",
    "inet_c_avg_acc = inet_c_acc_dfs['lc_with_patch'].groupby(['sev'])[['acc']].mean()\n",
    "inet_c_avg_acc.rename({'acc': 'patch-acc'}, axis='columns', inplace=True)\n",
    "acc_cls = inet_c_acc_dfs['lc_cls'].groupby(['sev'])[['acc']].mean()\n",
    "inet_c_avg_acc['cls-acc'] = acc_cls['acc']\n",
    "inet_c_avg_acc = inet_c_avg_acc.assign(dataset= lambda x: x.index)\n",
    "inet_c_avg_acc['dataset'] = inet_c_avg_acc['dataset'].apply(lambda x: 'inet-c-sev-' + str(x))\n",
    "print(inet_c_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "724c0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.04s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.62s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.65s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.70s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.64s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.59s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.64s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# calculate results for inet-r, inet-v2-70, inet-v2-mf, inet-v2-top \n",
    "\n",
    "ood_acc = dict() \n",
    "\n",
    "for model, model_name in [(lc_one_layer, 'lc_with_patch'), (lc_cls_token, 'lc_cls')]: \n",
    "    ood_acc[model_name] = dict()\n",
    "    for dname in dataloaders_dict.keys(): \n",
    "        if model_name == 'lc_with_patch':\n",
    "            ood_acc[model_name][dname] = calc_accuracy(model, 1, True, [dataloaders_dict[dname]], is_inet_c_data=False)[0]\n",
    "        else: \n",
    "            ood_acc[model_name][dname] = calc_accuracy(model, 1, False, [dataloaders_dict[dname]], is_inet_c_data=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store ood-acc results \n",
    "\n",
    "with open('/home/stud/afroehli/coding/dinov2_ood/storage/lc_cls_pretr_ood_dsets_res.pkl', 'wb') as pkl_fl:\n",
    "    pickle.dump(ood_acc, pkl_fl, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef4a428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patch-acc   cls-acc       dataset\n",
      "1   0.739889  0.677464  inet-c-sev-1\n",
      "2   0.673617  0.615355  inet-c-sev-2\n",
      "3   0.599708  0.548516  inet-c-sev-3\n",
      "4   0.497449  0.456148  inet-c-sev-4\n",
      "5   0.367285  0.338309  inet-c-sev-5\n",
      "0   0.817740  0.746440   inet_1k_val\n",
      "1   0.406200  0.424333        inet_r\n",
      "2   0.796900  0.729400    inet_v2_70\n",
      "3   0.727700  0.668600    inet_v2_mf\n",
      "4   0.832600  0.763500   inet_v2_top\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126944/1684277008.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_ood_df.rename(mapper={'acc': 'patch-acc'}, axis='columns', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# create DF for ood-acc-dict: (dset, model, acc)\n",
    "\n",
    "ood_acc_data = []\n",
    "for model_name in ood_acc.keys():\n",
    "    for dset_name in ood_acc[model_name].keys():\n",
    "        ood_acc_data.append((dset_name, model_name, ood_acc[model_name][dset_name]))\n",
    "\n",
    "ood_acc_df = pd.DataFrame(data=ood_acc_data, columns=['dataset', 'model', 'acc'])\n",
    "acc_with_patch = ood_acc_df.loc[ood_acc_df['model']=='lc_with_patch']\n",
    "acc_cls = ood_acc_df.loc[ood_acc_df['model']=='lc_cls']\n",
    "acc_cls = acc_cls.set_index([pd.Index([0, 1, 2, 3, 4])])\n",
    "acc_ood_df = acc_with_patch[['dataset', 'acc']]\n",
    "acc_ood_df.rename(mapper={'acc': 'patch-acc'}, axis='columns', inplace=True)\n",
    "acc_ood_df['cls-acc'] = acc_cls['acc']\n",
    "\n",
    "# concatenate inet-c results \n",
    "acc_ood_all = pd.concat([inet_c_avg_acc, acc_ood_df])\n",
    "print(acc_ood_all.head(n=15))\n",
    "\n",
    "with open('/home/stud/afroehli/coding/dinov2_ood/storage/ood_acc_results.csv', 'w', newline='') as csv_fl:\n",
    "    acc_ood_all.to_csv(csv_fl, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2Pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
