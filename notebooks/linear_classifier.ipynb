{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dinov2_ood_utilities.imagenet_tree import ImagenetSemanticInfo, ImagenetSemanticSubtree\n",
    "from dinov2_ood_utilities.custom_datasets import DictionaryDataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b54081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "# define device to use\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fd72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop node discovered\n"
     ]
    }
   ],
   "source": [
    "# util objects\n",
    "\n",
    "imagenet_info = ImagenetSemanticInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0651b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../resources/imagenet_1k_label_order.txt', 'r') as label_order_file:\n",
    "    inet_1k_labels = label_order_file.readlines()\n",
    "    inet_1k_labels = [label_order_line.split()[0] for label_order_line in inet_1k_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb116ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2172629/837691120.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretr_head = torch.load('../resources/pretrained_heads/dinov2_vits14_linear_head.pth')\n",
      "/tmp/ipykernel_2172629/837691120.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretr_head_big = torch.load('../resources/pretrained_heads/dinov2_vits14_linear4_head.pth')\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "pretr_head = torch.load('../resources/pretrained_heads/dinov2_vits14_linear_head.pth')\n",
    "pretr_head_big = torch.load('../resources/pretrained_heads/dinov2_vits14_linear4_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b860f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearClassifier(nn.Module): \n",
    "\n",
    "    def __init__(self, in_features = 384, out_features = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features= in_features, out_features=out_features),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     return cat((self.network.forward(x), self.network[0](x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad5bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all needed dataloaders\n",
    "\n",
    "dataloaders_dict = dict()\n",
    "\n",
    "for loader_name, loader_path in [('inet_1k_val', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_1k_val_pt_timm_trans.pkl'),\n",
    "                                 ('inet_r', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_r_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_70', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_70_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_mf', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_mf_plus_pt_timm_trans.pkl'),\n",
    "                                 ('inet_v2_top', '/home/stud/afroehli/coding/model_results/dinov2_vits14/inet_v2_top_plus_pt_timm_trans.pkl')]:\n",
    "    \n",
    "    with open(loader_path, 'rb') as pkl_fl:\n",
    "        dataloaders_dict[loader_name] = torch.utils.data.DataLoader(dataset=DictionaryDataset(data=pickle.load(pkl_fl), index_list=inet_1k_labels), \n",
    "                                                   batch_size=128, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0e3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_inet_c = dict()\n",
    "\n",
    "src_path = '/home/stud/afroehli/coding/model_results/dinov2_vits14/imagenet_c'\n",
    "inet_c_cor_types = os.listdir(src_path)\n",
    "\n",
    "for cor_type in inet_c_cor_types:\n",
    "    for sev in range(1, 6):\n",
    "        with open(f'{src_path}/{cor_type}/sev_{sev}.pkl', 'rb') as pkl_fl: \n",
    "            if sev == 1:\n",
    "                dataloaders_inet_c[cor_type] = dict()\n",
    "            dataloaders_inet_c[cor_type][sev] = torch.utils.data.DataLoader(dataset=DictionaryDataset(pickle.load(pkl_fl), inet_1k_labels), \n",
    "                                                           batch_size=128, shuffle=False, num_workers=8, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57b6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function \n",
    "# takes: model and list of dataloaders\n",
    "# returns: accuracy for each dataloader\n",
    "\n",
    "def cls_with_patch_one_layer(batch_samples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cat([torch.cat((batch_sample[3][1].squeeze(), batch_sample[3][0].squeeze())).unsqueeze(0)\n",
    "                                            for batch_sample in batch_samples])\n",
    "\n",
    "def cls_with_patch_one_layer_inet_c(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return torch.cat([torch.cat((batch_sample[1].squeeze(), batch_sample[0].squeeze())).unsqueeze(0) for batch_sample in batch_samples])\n",
    "\n",
    "def cls_without_patch_one_layer(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return torch.cat([batch_sample[3][1] for batch_sample in batch_samples])\n",
    "\n",
    "def cls_without_patch_one_layer_inet_c(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return torch.cat([batch_sample[1] for batch_sample in batch_samples])\n",
    "\n",
    "def cls_with_patch_four_layers(batch_samples: torch.Tensor) -> torch.Tensor: \n",
    "    return torch.cat([torch.cat(((torch.cat([cls_token for _, cls_token in batch_sample], dim=-1),\n",
    "                    batch_sample[3][0])), dim=-1)\n",
    "                    for batch_sample in batch_samples])\n",
    "\n",
    "def calc_accuracy(model: torch.nn.Sequential, n_layers: int, with_patch: bool, dataloaders: list, is_inet_c_data: bool) -> list: \n",
    "\n",
    "    if n_layers == 1:\n",
    "        if with_patch:\n",
    "            sample_transform = cls_with_patch_one_layer \n",
    "            if is_inet_c_data:\n",
    "                sample_transform = cls_with_patch_one_layer_inet_c \n",
    "        else:\n",
    "            sample_transform = cls_without_patch_one_layer \n",
    "            if is_inet_c_data:\n",
    "                sample_transform = cls_without_patch_one_layer_inet_c\n",
    "    elif n_layers == 4: \n",
    "        sample_transform = cls_with_patch_four_layers \n",
    "\n",
    "    if sample_transform == None:\n",
    "        raise ValueError('SampleTransform function could not be defined')\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for dataloader in (pbar := tqdm(dataloaders, ncols=100)):\n",
    "                  \n",
    "        total_preds = 0\n",
    "        total_preds_true = 0\n",
    "        for batch_samples, batch_labels in dataloader: \n",
    "            batch_samples_dev = sample_transform(batch_samples).to(device)\n",
    "            \n",
    "            model_pred = model(batch_samples_dev).cpu()\n",
    "\n",
    "            is_equal = model_pred.argmax(axis=1) == batch_labels.argmax(axis=1)\n",
    "            total_preds += is_equal.shape[0]\n",
    "            total_preds_true += is_equal.type(torch.float).sum().item()\n",
    "\n",
    "        accuracies.append(total_preds_true / total_preds)\n",
    "\n",
    "    return accuracies \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324a2fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=1920, out_features=1000, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definition of linear classifiers \n",
    "\n",
    "model_params_one_layer = OrderedDict()\n",
    "model_params_four_layers = OrderedDict()\n",
    "model_params_cls_token = OrderedDict()\n",
    "model_params_one_layer['network.0.weight'] = pretr_head['weight']\n",
    "model_params_one_layer['network.0.bias'] = pretr_head['bias']\n",
    "model_params_cls_token['network.0.weight'] = pretr_head['weight'][:,0:384]\n",
    "model_params_cls_token['network.0.bias'] = pretr_head['bias']\n",
    "model_params_four_layers['network.0.weight'] = pretr_head_big['weight']\n",
    "model_params_four_layers['network.0.bias'] = pretr_head_big['bias']\n",
    "\n",
    "lc_one_layer = LinearClassifier(in_features=768, out_features=1000)\n",
    "lc_cls_token = LinearClassifier(in_features=384, out_features=1000)\n",
    "lc_four_layer = LinearClassifier(in_features=1920, out_features=1000)\n",
    "\n",
    "lc_one_layer.load_state_dict(model_params_one_layer)\n",
    "lc_cls_token.load_state_dict(model_params_cls_token)\n",
    "lc_four_layer.load_state_dict(model_params_four_layers)\n",
    "\n",
    "lc_one_layer.eval()\n",
    "lc_cls_token.eval()\n",
    "lc_four_layer.eval()\n",
    "\n",
    "lc_one_layer.to(device)\n",
    "lc_cls_token.to(device)\n",
    "lc_four_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                              | 3/95 [00:15<07:48,  5.09s/it]Exception in thread Thread-129 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "\n",
      "    self.run()\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stud/afroehli/miniconda3/envs/dinov2Pre/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sev \u001b[38;5;129;01min\u001b[39;00m dataloaders_inet_c[cor_type].keys():\n\u001b[32m      8\u001b[39m         inet_c_dloader_list.append(dataloaders_inet_c[cor_type][sev])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m inet_c_acc_list = \u001b[43mcalc_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlc_cls_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minet_c_dloader_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inet_c_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m acc_res_i = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cor_type \u001b[38;5;129;01min\u001b[39;00m dataloaders_inet_c.keys():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mcalc_accuracy\u001b[39m\u001b[34m(model, n_layers, with_patch, dataloaders, is_inet_c_data)\u001b[39m\n\u001b[32m     44\u001b[39m total_preds = \u001b[32m0\u001b[39m\n\u001b[32m     45\u001b[39m total_preds_true = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_samples_dev\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_samples_dev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1402\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1404\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov2Pre/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Calculate acc for each Imagenet-C Val dataset \n",
    "\n",
    "inet_c_accuracies = dict()\n",
    "inet_c_dloader_list = []\n",
    "\n",
    "for cor_type in dataloaders_inet_c.keys():\n",
    "    for sev in dataloaders_inet_c[cor_type].keys():\n",
    "        inet_c_dloader_list.append(dataloaders_inet_c[cor_type][sev])\n",
    "\n",
    "inet_c_acc_list = calc_accuracy(lc_cls_token, 1, False, inet_c_dloader_list, is_inet_c_data=True)\n",
    "\n",
    "acc_res_i = 0\n",
    "for cor_type in dataloaders_inet_c.keys():\n",
    "    for sev in dataloaders_inet_c[cor_type].keys():\n",
    "        if sev == 1:\n",
    "            inet_c_accuracies[cor_type] = dict()\n",
    "        inet_c_accuracies[cor_type][sev] = inet_c_acc_list[acc_res_i]\n",
    "        acc_res_i += 1 \n",
    "\n",
    "with open('../results/lc_cls_pretr_inet_c_res.pkl', 'wb') as pkl_fl:\n",
    "    pickle.dump(inet_c_accuracies, pkl_fl, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aad00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset cor_type  sev          model      acc\n",
      "0  inet-c  spatter    1  lc_with_patch  0.78862\n",
      "1  inet-c  spatter    2  lc_with_patch  0.75822\n",
      "2  inet-c  spatter    3  lc_with_patch  0.71746\n",
      "3  inet-c  spatter    4  lc_with_patch  0.72546\n",
      "4  inet-c  spatter    5  lc_with_patch  0.67486\n",
      "  dataset cor_type  sev   model      acc\n",
      "0  inet-c  spatter    1  lc_cls  0.72488\n",
      "1  inet-c  spatter    2  lc_cls  0.69854\n",
      "2  inet-c  spatter    3  lc_cls  0.66534\n",
      "3  inet-c  spatter    4  lc_cls  0.66916\n",
      "4  inet-c  spatter    5  lc_cls  0.63050\n"
     ]
    }
   ],
   "source": [
    "# one DF with acc of all OOD datasets \n",
    "\n",
    "inet_c_acc_dfs = dict()\n",
    "\n",
    "for model, load_path in [('lc_with_patch', '../results/lc_one_lay_pretr_inet_c_res.pkl'),\n",
    "              ('lc_cls', '../results/lc_cls_pretr_inet_c_res.pkl')]: \n",
    "    # load dict with acc of 95 Inet-C Datasets \n",
    "    with open(load_path, 'rb') as pkl_fl:\n",
    "        inet_c_acc_results = pickle.load(pkl_fl) \n",
    "\n",
    "    # create tuples with form: (dataset, cor, sev, model, acc)\n",
    "    inet_c_df_data = []\n",
    "    for cor_type in inet_c_acc_results.keys(): \n",
    "        for sev in inet_c_acc_results[cor_type].keys(): \n",
    "            acc = inet_c_acc_results[cor_type][sev]\n",
    "            inet_c_df_data.append(('inet-c', cor_type, sev, model, acc))\n",
    "\n",
    "    # create DF \n",
    "    inet_c_acc_dfs[model] = inet_c_acc_df_with_patch = pd.DataFrame(data=inet_c_df_data, columns=['dataset', 'cor_type', 'sev', 'model', 'acc'])\n",
    "    print(inet_c_acc_dfs[model].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     patch-acc   cls-acc       dataset\n",
      "sev                                   \n",
      "1     0.739889  0.677464  inet-c-sev-1\n",
      "2     0.673617  0.615355  inet-c-sev-2\n",
      "3     0.599708  0.548516  inet-c-sev-3\n",
      "4     0.497449  0.456148  inet-c-sev-4\n",
      "5     0.367285  0.338309  inet-c-sev-5\n"
     ]
    }
   ],
   "source": [
    "# bring inet-c-acc-DF in form (datasetname + sev, model-1-acc, model-2-acc)\n",
    "\n",
    "\n",
    "inet_c_avg_acc = inet_c_acc_dfs['lc_with_patch'].groupby(['sev'])[['acc']].mean()\n",
    "inet_c_avg_acc.rename({'acc': 'patch-acc'}, axis='columns', inplace=True)\n",
    "acc_cls = inet_c_acc_dfs['lc_cls'].groupby(['sev'])[['acc']].mean()\n",
    "inet_c_avg_acc['cls-acc'] = acc_cls['acc']\n",
    "inet_c_avg_acc = inet_c_avg_acc.assign(dataset= lambda x: x.index)\n",
    "inet_c_avg_acc['dataset'] = inet_c_avg_acc['dataset'].apply(lambda x: 'inet-c-sev-' + str(x))\n",
    "print(inet_c_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.04s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.62s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.65s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.70s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.64s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.59s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.64s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# calculate results for inet-r, inet-v2-70, inet-v2-mf, inet-v2-top \n",
    "\n",
    "ood_acc = dict() \n",
    "\n",
    "for model, model_name in [(lc_one_layer, 'lc_with_patch'), (lc_cls_token, 'lc_cls')]: \n",
    "    ood_acc[model_name] = dict()\n",
    "    for dname in dataloaders_dict.keys(): \n",
    "        if model_name == 'lc_with_patch':\n",
    "            ood_acc[model_name][dname] = calc_accuracy(model, 1, True, [dataloaders_dict[dname]], is_inet_c_data=False)[0]\n",
    "        else: \n",
    "            ood_acc[model_name][dname] = calc_accuracy(model, 1, False, [dataloaders_dict[dname]], is_inet_c_data=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store ood-acc results \n",
    "\n",
    "with open('../results/lc_cls_pretr_ood_dsets_res.pkl', 'wb') as pkl_fl:\n",
    "    pickle.dump(ood_acc, pkl_fl, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patch-acc   cls-acc       dataset\n",
      "1   0.739889  0.677464  inet-c-sev-1\n",
      "2   0.673617  0.615355  inet-c-sev-2\n",
      "3   0.599708  0.548516  inet-c-sev-3\n",
      "4   0.497449  0.456148  inet-c-sev-4\n",
      "5   0.367285  0.338309  inet-c-sev-5\n",
      "0   0.817740  0.746440   inet_1k_val\n",
      "1   0.406200  0.424333        inet_r\n",
      "2   0.796900  0.729400    inet_v2_70\n",
      "3   0.727700  0.668600    inet_v2_mf\n",
      "4   0.832600  0.763500   inet_v2_top\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126944/1684277008.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_ood_df.rename(mapper={'acc': 'patch-acc'}, axis='columns', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# create DF for ood-acc-dict: (dset, model, acc)\n",
    "\n",
    "ood_acc_data = []\n",
    "for model_name in ood_acc.keys():\n",
    "    for dset_name in ood_acc[model_name].keys():\n",
    "        ood_acc_data.append((dset_name, model_name, ood_acc[model_name][dset_name]))\n",
    "\n",
    "ood_acc_df = pd.DataFrame(data=ood_acc_data, columns=['dataset', 'model', 'acc'])\n",
    "acc_with_patch = ood_acc_df.loc[ood_acc_df['model']=='lc_with_patch']\n",
    "acc_cls = ood_acc_df.loc[ood_acc_df['model']=='lc_cls']\n",
    "acc_cls = acc_cls.set_index([pd.Index([0, 1, 2, 3, 4])])\n",
    "acc_ood_df = acc_with_patch[['dataset', 'acc']]\n",
    "acc_ood_df.rename(mapper={'acc': 'patch-acc'}, axis='columns', inplace=True)\n",
    "acc_ood_df['cls-acc'] = acc_cls['acc']\n",
    "\n",
    "# concatenate inet-c results \n",
    "acc_ood_all = pd.concat([inet_c_avg_acc, acc_ood_df])\n",
    "print(acc_ood_all.head(n=15))\n",
    "\n",
    "with open('../results/ood_acc_results.csv', 'w', newline='') as csv_fl:\n",
    "    acc_ood_all.to_csv(csv_fl, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2Pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
